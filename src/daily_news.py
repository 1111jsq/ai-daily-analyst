"""
æ¯æ—¥ AI æ–°é—»æŠ“å–ä¸åˆ†æ
"""
import os
import json
import logging
import re
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Optional
from collections import Counter

from tavily import TavilyClient

from .config import (
    TAVILY_API_KEY,
    SEARCH_DEPTH,
    MAX_RESULTS,
    DAILY_TOPICS,
    DATA_DIR,
    OUTPUT_DIR,
    LOG_LEVEL,
    TECH_CATEGORIES,
)

logging.basicConfig(
    level=getattr(logging, LOG_LEVEL),
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)


class DailyNewsCollector:
    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or TAVILY_API_KEY
        if not self.api_key:
            raise ValueError("Tavily API key is required")
        self.client = TavilyClient(api_key=self.api_key)

    def search_topic(self, topic: str, days: int = 1) -> List[Dict]:
        query = f"{topic} AI news"
        try:
            results = self.client.search(
                query=query,
                search_depth=SEARCH_DEPTH,
                max_results=MAX_RESULTS,
                include_answer=True,
                include_raw_content=False,
            )
            logger.info(f"æœç´¢ '{topic}' è·å¾— {len(results.get('results', []))} æ¡ç»“æœ")
            return results
        except Exception as e:
            logger.error(f"æœç´¢ '{topic}' å¤±è´¥: {e}")
            return {}

    def collect_daily_news(self, date: Optional[datetime] = None) -> Dict:
        date = date or datetime.now()
        date_str = date.strftime("%Y-%m-%d")
        
        all_results = {
            "date": date_str,
            "topics": {},
            "ai_answer": "",
            "articles": [],
        }

        for topic in DAILY_TOPICS:
            logger.info(f"æ­£åœ¨æœç´¢: {topic}")
            result = self.search_topic(topic)
            if result:
                all_results["topics"][topic] = result
                for item in result.get("results", []):
                    article = {
                        "title": item.get("title", ""),
                        "url": item.get("url", ""),
                        "content": item.get("content", "")[:200],
                        "score": item.get("score", 0),
                        "topic": topic,
                    }
                    all_results["articles"].append(article)

        if all_results["topics"]:
            first_topic = list(all_results["topics"].values())[0]
            all_results["ai_answer"] = first_topic.get("answer", "")

        return all_results

    def save_news(self, news_data: Dict) -> Path:
        date_str = news_data["date"]
        filename = f"news_{date_str}.json"
        filepath = DATA_DIR / filename
        
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(news_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æ–°é—»æ•°æ®å·²ä¿å­˜åˆ°: {filepath}")
        return filepath


def categorize_article(title: str, content: str = "") -> str:
    text = (title + " " + content).lower()
    for category, keywords in TECH_CATEGORIES.items():
        for kw in keywords:
            if kw.lower() in text:
                return category
    return "å…¶ä»–"


class DailyAnalyst:
    def __init__(self, news_collector: DailyNewsCollector):
        self.collector = news_collector

    def categorize_articles(self, articles: List[Dict]) -> Dict[str, List[Dict]]:
        categorized = {cat: [] for cat in TECH_CATEGORIES.keys()}
        categorized["å…¶ä»–"] = []
        
        for article in articles:
            category = categorize_article(article.get("title", ""), article.get("content", ""))
            categorized[category].append(article)
        
        return {k: v for k, v in categorized.items() if v}

    def generate_trend_summary(self, categorized: Dict[str, List[Dict]]) -> str:
        summaries = []
        category_counts = {cat: len(articles) for cat, articles in categorized.items()}
        
        for category, articles in categorized.items():
            if not articles:
                continue
            top_articles = sorted(articles, key=lambda x: x.get("score", 0), reverse=True)[:3]
            summaries.append(f"**{category}** ({len(articles)}ç¯‡)")
            for art in top_articles:
                summaries.append(f"- {art['title'][:60]}...")
            summaries.append("")
        
        return "\n".join(summaries)

    def generate_analysis(self, news_data: Dict) -> str:
        date_str = news_data["date"]
        articles = news_data.get("articles", [])
        ai_answer = news_data.get("ai_answer", "")

        categorized = self.categorize_articles(articles)
        
        top_articles = sorted(articles, key=lambda x: x.get("score", 0), reverse=True)[:10]

        content = f"""# æ¯æ—¥ AI åŠ¨æ€åˆ†æ - {date_str}

## ä»Šæ—¥ AI é¢†åŸŸè¦é—»

"""

        if ai_answer:
            content += f"**AI æ€»ç»“ï¼š**{ai_answer}\n\n"

        content += f"## é‡ç‚¹æ–°é—» ({len(articles)}ç¯‡)\n\n"

        for i, article in enumerate(top_articles, 1):
            cat = categorize_article(article.get("title", ""))
            content += f"### {i}. {article['title']}\n"
            content += f"**æ¥æºï¼š** {article.get('url', 'Unknown')[:50]}...\n"
            content += f"**ç±»åˆ«ï¼š** {cat}\n"
            if article.get("content"):
                content += f"\n{article['content']}\n"
            content += "\n---\n\n"

        content += self.generate_trend_summary(categorized)

        content += f"""
## ç±»åˆ«åˆ†å¸ƒ

| ç±»åˆ« | æ–‡ç« æ•° |
|:---|:---:|
"""
        for cat, arts in sorted(categorized.items(), key=lambda x: len(x[1]), reverse=True):
            content += f"| {cat} | {len(arts)} |\n"

        content += f"""
---

## åˆ†æç»“è¯­

ä»Šæ—¥ AI é¢†åŸŸå…±æ”¶é›†åˆ° **{len(articles)}** æ¡ç›¸å…³æ–°é—»ï¼Œæ¶µç›– {len(categorized)} ä¸ªæŠ€æœ¯é¢†åŸŸã€‚

**æ ¸å¿ƒå…³æ³¨ï¼š** æœ¬æ—¥æ–°é—»ä¸»è¦é›†ä¸­åœ¨ {', '.join(list(categorized.keys())[:3])} ç­‰é¢†åŸŸï¼Œå±•ç°å‡º{"ã€".join(list(categorized.keys())[:2])}æŠ€æœ¯çš„å¿«é€Ÿå‘å±•è¶‹åŠ¿ã€‚

---
*æœ¬æ–‡ç”± AI è‡ªåŠ¨ç”Ÿæˆ | å‘å¸ƒæ—¥æœŸï¼š{date_str}*
"""

        return content

    def save_article(self, content: str, date_str: str) -> Path:
        filename = f"article_{date_str}.md"
        filepath = OUTPUT_DIR / filename
        
        with open(filepath, "w", encoding="utf-8") as f:
            f.write(content)
        
        logger.info(f"åˆ†ææ–‡ç« å·²ä¿å­˜åˆ°: {filepath}")
        return filepath

    def run(self, date: Optional[datetime] = None) -> Dict:
        date = date or datetime.now()
        date_str = date.strftime("%Y-%m-%d")
        
        logger.info(f"å¼€å§‹ç”Ÿæˆ {date_str} çš„æ¯æ—¥åˆ†æ...")
        
        news_data = self.collector.collect_daily_news(date)
        self.collector.save_news(news_data)
        
        article_content = self.generate_analysis(news_data)
        article_path = self.save_article(article_content, date_str)
        
        return {
            "date": date_str,
            "article_path": str(article_path),
            "articles_count": len(news_data.get("articles", [])),
        }


def main():
    collector = DailyNewsCollector()
    analyst = DailyAnalyst(collector)
    result = analyst.run()
    print(f"âœ… æ¯æ—¥åˆ†æå®Œæˆï¼")
    print(f"ğŸ“… æ—¥æœŸ: {result['date']}")
    print(f"ğŸ“„ æ–‡ç« : {result['article_path']}")
    print(f"ğŸ“Š æ”¶é›†æ–‡ç« æ•°: {result['articles_count']}")


if __name__ == "__main__":
    main()
