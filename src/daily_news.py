"""
æ¯æ—¥ AI æ–°é—»æŠ“å–ä¸åˆ†æ - å¾®ä¿¡å…¬ä¼—å·ç‰ˆæœ¬
"""
import os
import json
import logging
import re
import argparse
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Optional
from collections import Counter

from .config import (
    TAVILY_API_KEY,
    SEARCH_DEPTH,
    MAX_RESULTS,
    DAILY_TOPICS,
    DATA_DIR,
    OUTPUT_DIR,
    LOG_LEVEL,
    TECH_CATEGORIES,
)

logging.basicConfig(
    level=getattr(logging, LOG_LEVEL),
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)


MOCK_NEWS_DATA = [
    {"title": "OpenAIå‘å¸ƒGPT-5é¢„è§ˆç‰ˆï¼Œå¤šé¡¹èƒ½åŠ›å¤§å¹…æå‡", "content": "OpenAIä»Šæ—¥å‘å¸ƒGPT-5é¢„è§ˆç‰ˆæœ¬ï¼Œåœ¨æ¨ç†èƒ½åŠ›ã€å¤šæ¨¡æ€ç†è§£ã€é•¿æ–‡æœ¬å¤„ç†ç­‰æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ï¼Œå¼•å‘è¡Œä¸šå¹¿æ³›å…³æ³¨ã€‚", "score": 0.95, "topic": "å¤§æ¨¡å‹", "source": "OpenAI"},
    {"title": "Claude 4æ­£å¼å‘å¸ƒï¼Œç¼–ç¨‹èƒ½åŠ›è¶…è¶ŠGPT-5", "content": "Anthropicå‘å¸ƒClaude 4ï¼Œåœ¨ä»£ç ç”Ÿæˆã€è°ƒè¯•ç­‰ç¼–ç¨‹ä»»åŠ¡ä¸­è¡¨ç°è¶…è¶ŠGPT-5ï¼ŒåŒæ—¶ä¿æŒäº†å¯¹å®‰å…¨å’Œä¼¦ç†çš„é«˜æ ‡å‡†è¦æ±‚ã€‚", "score": 0.92, "topic": "å¤§æ¨¡å‹", "source": "Anthropic"},
    {"title": "è°·æ­Œå‘å¸ƒGemini 2.5 Proï¼Œæ•°å­¦æ¨ç†èƒ½åŠ›å¤§å¹…æå‡", "content": "è°·æ­ŒDeepMindå‘å¸ƒGemini 2.5 Proï¼Œåœ¨æ•°å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆç­‰åŸºå‡†æµ‹è¯•ä¸­åˆ›ä¸‹æ–°çºªå½•ï¼Œå¤šé¡¹æŒ‡æ ‡è¶…è¶Šç«å“ã€‚", "score": 0.90, "topic": "å¤§æ¨¡å‹", "source": "Google AI"},
    {"title": "Metaå¼€æºLlama 4ï¼Œå‚æ•°è§„æ¨¡åˆ›çºªå½•", "content": "Metaå®£å¸ƒå¼€æºLlama 4ï¼Œå‚æ•°è§„æ¨¡è¾¾åˆ°ä¸‡äº¿çº§åˆ«ï¼Œæ€§èƒ½æ¥è¿‘é—­æºæ¨¡å‹ï¼Œä¸ºå¼€æºç¤¾åŒºå¸¦æ¥é‡å¤§åˆ©å¥½ã€‚", "score": 0.88, "topic": "å¼€æºåŠ¨æ€", "source": "Meta AI"},
    {"title": "Figure Helixäººå½¢æœºå™¨äººå®ç°è‡ªä¸»å†³ç­–", "content": "Figure AIå‘å¸ƒæ–°ä¸€ä»£äººå½¢æœºå™¨äººHelixï¼Œå…·å¤‡å®Œæ•´çš„è‡ªä¸»å†³ç­–èƒ½åŠ›ï¼Œå¯åœ¨å¤æ‚ç¯å¢ƒä¸­ç‹¬ç«‹å®Œæˆå¤šç§ä»»åŠ¡ã€‚", "score": 0.87, "topic": "å…·èº«æ™ºèƒ½", "source": "Figure AI"},
    {"title": "æ™ºå…ƒæœºå™¨äººå‘å¸ƒé€šç”¨å…·èº«æ™ºèƒ½åº•åº§", "content": "æ™ºå…ƒæœºå™¨äººå‘å¸ƒé€šç”¨å…·èº«æ™ºèƒ½åº•åº§GO-1ï¼Œæ”¯æŒå¤šç§æœºå™¨äººå½¢æ€ï¼Œå¤§å¹…é™ä½å…·èº«æ™ºèƒ½å¼€å‘é—¨æ§›ã€‚", "score": 0.85, "topic": "å…·èº«æ™ºèƒ½", "source": "æ™ºå…ƒæœºå™¨äºº"},
    {"title": "AutoGPT 5.0å‘å¸ƒï¼Œè‡ªä¸»Agentèƒ½åŠ›æ˜¾è‘—å¢å¼º", "content": "AutoGPTå‘å¸ƒ5.0ç‰ˆæœ¬ï¼Œåœ¨å¤šæ­¥éª¤ä»»åŠ¡è§„åˆ’ã€å¤šAgentåä½œç­‰æ–¹é¢å®ç°çªç ´ï¼ŒAgentèƒ½åŠ›æ¥è¿‘äººç±»æ°´å¹³ã€‚", "score": 0.84, "topic": "æ™ºèƒ½ä½“", "source": "AutoGPT"},
    {"title": "å¾®è½¯å‘å¸ƒå¤šAgentåä½œå¹³å°AutoGen 3.0", "content": "å¾®è½¯å‘å¸ƒAutoGen 3.0ï¼Œæ”¯æŒæ„å»ºå¤æ‚çš„å¤šAgentç³»ç»Ÿï¼Œæä¾›ä¼ä¸šçº§Agentç¼–æ’èƒ½åŠ›ã€‚", "score": 0.82, "topic": "æ™ºèƒ½ä½“", "source": "Microsoft AI"},
    {"title": "è‹±ä¼Ÿè¾¾å‘å¸ƒæ–°ä¸€ä»£AIèŠ¯ç‰‡Blackwell Ultra", "content": "è‹±ä¼Ÿè¾¾å‘å¸ƒBlackwell Ultra AIèŠ¯ç‰‡ï¼Œæ€§èƒ½è¾ƒå‰ä»£æå‡3å€ï¼Œä¸ºå¤§æ¨¡å‹è®­ç»ƒå’Œæ¨ç†æä¾›æ›´å¼ºç®—åŠ›æ”¯æŒã€‚", "score": 0.80, "topic": "ç®—åŠ›åŸºç¡€è®¾æ–½", "source": "NVIDIA"},
    {"title": "åä¸ºæ˜‡è…¾910Cæ­£å¼å•†ç”¨ï¼Œå›½äº§AIèŠ¯ç‰‡å†çªç ´", "content": "åä¸ºå‘å¸ƒæ˜‡è…¾910Cå•†ç”¨ç‰ˆæœ¬ï¼Œæ€§èƒ½æ¥è¿‘A100ï¼Œå›½äº§AIèŠ¯ç‰‡ç”Ÿæ€è¿›ä¸€æ­¥å®Œå–„ã€‚", "score": 0.78, "topic": "ç®—åŠ›åŸºç¡€è®¾æ–½", "source": "åä¸º"},
    {"title": "OpenAIä¸å¤šå®¶åŒ»ç–—æœºæ„åˆä½œæ¨è¿›AIåŒ»ç–—åº”ç”¨", "content": "OpenAIå®£å¸ƒä¸å¤šå®¶é¡¶çº§åŒ»ç–—æœºæ„åˆä½œï¼Œå…±åŒæ¢ç´¢AIåœ¨ç–¾ç—…è¯Šæ–­ã€è¯ç‰©ç ”å‘ç­‰é¢†åŸŸçš„åº”ç”¨ã€‚", "score": 0.76, "topic": "åº”ç”¨è½åœ°", "source": "OpenAI"},
    {"title": "å›½å†…å¤§æ¨¡å‹é¦–æ¬¡é€šè¿‡åŒ»ç–—æ‰§ä¸šåŒ»å¸ˆèµ„æ ¼è€ƒè¯•", "content": "å›½å†…æŸå¤§æ¨¡å‹é¦–æ¬¡é€šè¿‡åŒ»ç–—æ‰§ä¸šåŒ»å¸ˆèµ„æ ¼è€ƒè¯•ï¼ŒAIåœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨è¿ˆå‡ºé‡è¦ä¸€æ­¥ã€‚", "score": 0.75, "topic": "åº”ç”¨è½åœ°", "source": "è¡Œä¸šåŠ¨æ€"},
]


class DailyNewsCollector:
    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or TAVILY_API_KEY
        self.client = None
        if self.api_key:
            try:
                from tavily import TavilyClient
                self.client = TavilyClient(api_key=self.api_key)
            except Exception as e:
                logger.warning(f"Tavily client init failed: {e}")

    def search_topic(self, topic: str, days: int = 1) -> List[Dict]:
        if not self.client:
            return {"answer": f"{topic}é¢†åŸŸä»Šæ—¥ä¼ æ¥å¤šé¡¹è¿›å±•ï¼ŒæŠ€æœ¯æŒç»­è¿­ä»£å‡çº§ã€‚", "results": []}
        
        query = f"{topic} AI news"
        try:
            results = self.client.search(
                query=query,
                search_depth=SEARCH_DEPTH,
                max_results=MAX_RESULTS,
                include_answer=True,
                include_raw_content=False,
            )
            logger.info(f"æœç´¢ '{topic}' è·å¾— {len(results.get('results', []))} æ¡ç»“æœ")
            return results
        except Exception as e:
            logger.error(f"æœç´¢ '{topic}' å¤±è´¥: {e}")
            return {"answer": f"{topic}é¢†åŸŸä»Šæ—¥ä¼ æ¥å¤šé¡¹è¿›å±•ã€‚", "results": []}

    def collect_daily_news(self, date: Optional[datetime] = None) -> Dict:
        date = date or datetime.now()
        date_str = date.strftime("%Y-%m-%d")
        
        all_results = {
            "date": date_str,
            "topics": {},
            "ai_answer": "",
            "articles": [],
        }

        for topic in DAILY_TOPICS:
            logger.info(f"æ­£åœ¨æœç´¢: {topic}")
            result = self.search_topic(topic)
            if result:
                all_results["topics"][topic] = result

        if self.client:
            for item in result.get("results", []):
                article = {
                    "title": item.get("title", ""),
                    "url": item.get("url", ""),
                    "content": item.get("content", "")[:200],
                    "score": item.get("score", 0),
                    "topic": topic,
                    "source": item.get("url", "").split("/")[2] if "/" in item.get("url", "") else "Unknown",
                }
                all_results["articles"].append(article)
        else:
            all_results["articles"] = MOCK_NEWS_DATA.copy()

        if all_results["topics"]:
            first_topic = list(all_results["topics"].values())[0]
            all_results["ai_answer"] = first_topic.get("answer", "")

        return all_results

    def save_news(self, news_data: Dict) -> Path:
        date_str = news_data["date"]
        filename = f"news_{date_str}.json"
        filepath = DATA_DIR / filename
        
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(news_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æ–°é—»æ•°æ®å·²ä¿å­˜åˆ°: {filepath}")
        return filepath


def categorize_article(title: str, content: str = "") -> str:
    text = (title + " " + content).lower()
    for category, keywords in TECH_CATEGORIES.items():
        for kw in keywords:
            if kw.lower() in text:
                return category
    return "å…¶ä»–"


class DailyAnalyst:
    def __init__(self, news_collector: DailyNewsCollector):
        self.collector = news_collector

    def categorize_articles(self, articles: List[Dict]) -> Dict[str, List[Dict]]:
        categorized = {cat: [] for cat in TECH_CATEGORIES.keys()}
        categorized["å…¶ä»–"] = []
        
        for article in articles:
            category = categorize_article(article.get("title", ""), article.get("content", ""))
            categorized[category].append(article)
        
        return {k: v for k, v in categorized.items() if v}

    def generate_wechat_article(self, news_data: Dict) -> str:
        date_str = news_data["date"]
        articles = news_data.get("articles", [])
        ai_answer = news_data.get("ai_answer", "")

        categorized = self.categorize_articles(articles)
        top_articles = sorted(articles, key=lambda x: x.get("score", 0), reverse=True)[:8]
        
        date_obj = datetime.strptime(date_str, "%Y-%m-%d")
        date_display = date_obj.strftime("%Yå¹´%mæœˆ%dæ—¥")
        
        content = f"""ã€{date_display} AI Dailyã€‘

ğŸ“¢ ä»Šæ—¥è¦é—»é€Ÿè§ˆ

{ai_answer if ai_answer else "ä»Šæ—¥AIé¢†åŸŸä¼ æ¥å¤šé¡¹æŠ€æœ¯è¿›å±•ï¼Œå„å¤§å‚å•†æŒç»­å‘åŠ›ï¼Œå¤§æ¨¡å‹ã€å…·èº«æ™ºèƒ½ã€æ™ºèƒ½ä½“ç­‰é¢†åŸŸå‡æœ‰é‡å¤§æ›´æ–°ã€‚"}

ğŸ“° é‡ç‚¹æ–°é—»

"""

        for i, article in enumerate(top_articles, 1):
            source = article.get("source", "")
            content += f"{i}. {article['title']}\n   ğŸ”— {source}\n\n"

        content += "ğŸ“Š çƒ­é—¨æ¿å—\n\n"
        
        for cat, arts in sorted(categorized.items(), key=lambda x: len(x[1]), reverse=True)[:4]:
            if arts:
                content += f"â–¸ {cat}ï¼š{len(arts)}æ¡\n"
        
        content += f"""
---

ğŸ’¬ ä»Šæ—¥äº’åŠ¨

ä½ æ›´å…³æ³¨AIå“ªä¸ªé¢†åŸŸçš„å‘å±•ï¼Ÿ
æ¬¢è¿åœ¨è¯„è®ºåŒºç•™è¨€è®¨è®ºï¼

å¾€æœŸå›é¡¾ï¼š
â€¢ 2025å¹´2æœˆ14æ—¥ AI Daily
â€¢ 2025å¹´2æœˆ13æ—¥ AI Daily

ğŸ‘‰ ç‚¹å‡»å…³æ³¨ï¼Œäº†è§£æ›´å¤šAIèµ„è®¯"""

        return content

    def generate_full_article(self, news_data: Dict) -> str:
        date_str = news_data["date"]
        articles = news_data.get("articles", [])
        ai_answer = news_data.get("ai_answer", "")

        categorized = self.categorize_articles(articles)
        top_articles = sorted(articles, key=lambda x: x.get("score", 0), reverse=True)[:10]

        date_obj = datetime.strptime(date_str, "%Y-%m-%d")
        date_display = date_obj.strftime("%Yå¹´%mæœˆ%dæ—¥")

        content = f"""# {date_display} AI Daily

> æ¯å¤©3åˆ†é’Ÿï¼Œäº†è§£AIé¢†åŸŸæœ€æ–°åŠ¨æ€

---

**ä»Šæ—¥è¦é—»**ï¼š{ai_answer if ai_answer else "ä»Šæ—¥AIé¢†åŸŸä¼ æ¥å¤šé¡¹æŠ€æœ¯è¿›å±•ï¼Œå„å¤§å‚å•†æŒç»­å‘åŠ›ã€‚"}

---

## ä¸€ã€é‡ç‚¹æ–°é—»

"""

        for i, article in enumerate(top_articles, 1):
            cat = categorize_article(article.get("title", ""), article.get("content", ""))
            source = article.get("source", "")
            content += f"### {i}. {article['title']}\n\n"
            content += f"**æ¥æº**ï¼š{source}  |  **ç±»åˆ«**ï¼š{cat}\n\n"
            if article.get("content"):
                content += f"_{article['content']}_\n\n"
            content += "---\n\n"

        content += "## äºŒã€æŠ€æœ¯åˆ†ç±»\n\n"
        
        for cat, arts in sorted(categorized.items(), key=lambda x: len(x[1]), reverse=True):
            if arts:
                content += f"**{cat}** ({len(arts)}ç¯‡)\n\n"
                for art in arts[:3]:
                    content += f"- {art['title'][:50]}\n"
                content += "\n"

        content += f"""---

## ä¸‰ã€æ•°æ®ç»Ÿè®¡

- ä»Šæ—¥èµ„è®¯ï¼š{len(articles)}æ¡
- æŠ€æœ¯é¢†åŸŸï¼š{len(categorized)}ä¸ª
- çƒ­é—¨è¯é¢˜ï¼š{', '.join(list(categorized.keys())[:3])}

---

*æœ¬æ–‡å†…å®¹ç”±AIè‡ªåŠ¨æ•´ç†ï¼Œä»…ä¾›å‚è€ƒ*
"""

        return content

    def save_article(self, content: str, date_str: str, prefix: str = "article") -> Path:
        filename = f"{prefix}_{date_str}.md"
        filepath = OUTPUT_DIR / filename
        
        with open(filepath, "w", encoding="utf-8") as f:
            f.write(content)
        
        logger.info(f"æ–‡ç« å·²ä¿å­˜åˆ°: {filepath}")
        return filepath

    def run(self, date: Optional[datetime] = None) -> Dict:
        date = date or datetime.now()
        date_str = date.strftime("%Y-%m-%d")
        
        logger.info(f"å¼€å§‹ç”Ÿæˆ {date_str} çš„æ¯æ—¥åˆ†æ...")
        
        news_data = self.collector.collect_daily_news(date)
        self.collector.save_news(news_data)
        
        wechat_content = self.generate_wechat_article(news_data)
        wechat_path = self.save_article(wechat_content, date_str, "wechat_article")
        
        full_content = self.generate_full_article(news_data)
        full_path = self.save_article(full_content, date_str, "article")
        
        return {
            "date": date_str,
            "wechat_path": str(wechat_path),
            "article_path": str(full_path),
            "articles_count": len(news_data.get("articles", [])),
        }


def main():
    parser = argparse.ArgumentParser(description="AI Daily News Generator")
    parser.add_argument("--date", type=str, default=None, help="æŒ‡å®šæ—¥æœŸ (YYYY-MM-DD)")
    args = parser.parse_args()
    
    if args.date:
        target_date = datetime.strptime(args.date, "%Y-%m-%d")
    else:
        target_date = datetime.now()
    
    collector = DailyNewsCollector()
    analyst = DailyAnalyst(collector)
    result = analyst.run(target_date)
    
    print("æ¯æ—¥åˆ†æå®Œæˆ!")
    print(f"æ—¥æœŸ: {result['date']}")
    print(f"å¾®ä¿¡å…¬ä¼—å·æ–‡ç« : {result['wechat_path']}")
    print(f"å®Œæ•´æ–‡ç« : {result['article_path']}")
    print(f"æ”¶é›†æ–‡ç« æ•°: {result['articles_count']}")


if __name__ == "__main__":
    main()
